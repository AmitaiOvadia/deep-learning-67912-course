# -*- coding: utf-8 -*-
"""Ex1 GPT-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wkN3ht8aiTmhwUjaU_R4lt8gc2M6Gbd1
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/DL course/EX1/minGPT-master/minGPT-master')
import mingpt
import torch
from torch.utils.data import Dataset, DataLoader,TensorDataset
import torch.nn as nn
from torch.nn import functional as F

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

path = "/content/drive/MyDrive/DL course/EX1/Alice's Adventures in Wonderland.txt"
with open(path, "r") as f:
    text = f.read()

"""Create dataset"""

from mingpt import bpe

tokenizer = bpe.BPETokenizer()
idx = tokenizer(text, return_tensors='pt')

e = bpe.get_encoder()
tokens = torch.tensor(e.encode(text))

block_size = 64

# Define a custom dataset class that returns X and Y for each index
class TextDataset(Dataset):
    def __init__(self, tokens):
        # tokens is a PyTorch tensor of size N that contains the encoded text
        self.tokens = tokens
    
    def __len__(self):
        # The length of the dataset is the number of possible blocks
        return len(self.tokens) - block_size
    
    def __getitem__(self, idx):
        # The input X is the block of tokens from idx to idx + block_size
        X = self.tokens[idx : idx + block_size]
        # The target Y is the block of tokens from idx + 1 to idx + block_size + 1
        Y = self.tokens[idx + 1 : idx + block_size + 1]
        return X, Y

def create_dataset(tokens):
  dataset_size = len(tokens) - block_size
  all_X = []
  all_Y = []
  X_datset = torch.zeros(dataset_size, block_size)
  Y_datset = torch.zeros(dataset_size, block_size)
  
  for i in range(len(tokens) - block_size):
    X = tokens[i: i + block_size]
    Y = tokens[i + 1: i + block_size + 1] 
    X_datset[i, :] = X
    Y_datset[i, :] = Y
  X_datset = X_datset[:-1, :].long().to(torch.device('cpu')).pin_memory()
  Y_datset = Y_datset[1:, :].long().to(torch.device('cpu')).pin_memory()
  print(X_datset.shape)
  print(Y_datset.shape)

  return TensorDataset(X_datset, Y_datset)
# Create an instance of the dataset class with the test set of tokens
dataset = create_dataset(tokens)

# dataset = [tokens[i: i+block_size] for i in range(len(tokens) - block_size)]
# X_dataset = torch.stack(dataset[:-1]).to(torch.device('cpu')).pin_memory()
# Y_dataset = torch.stack(dataset[1:]).to(torch.device('cpu')).pin_memory()
# dataset = TensorDataset(X_dataset, Y_dataset)

"""# Q1 train and save a model"""

# create a GPT instance
from mingpt.model import GPT
import matplotlib.pyplot as plt
from mingpt.trainer import Trainer
GPT_TYPE = 'gpt2'
losses = []

def load_gpt_model():
  VOCAB_SIZE = 50257
  model_config = GPT.get_default_config()
  model_config.model_type = GPT_TYPE
  model_config.vocab_size = VOCAB_SIZE
  model_config.block_size = block_size
  return GPT(model_config)

def batch_end_callback(trainer):
    if trainer.iter_num % 10 == 0:
        print(f"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}")
        losses.append(trainer.loss.item())



def train_model():
    model = load_gpt_model()
    # create a Trainer object
    train_config = Trainer.get_default_config()
    train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster
    train_config.max_iters = 5000
    trainer = Trainer(train_config, model, dataset)
    trainer.set_callback('on_batch_end', batch_end_callback)
    trainer.run()
    torch.save(model.state_dict(), f'/content/drive/MyDrive/DL course/EX1/trained models/model_{GPT_TYPE}_iters_{train_config.max_iters}.pth')

train_model()

def plot_loss(losses):
  plt.plot(losses, label='Training loss')
  plt.legend()
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.title('Training Loss')
  plt.show()

plot_loss(losses)

"""New GPT for inversion"""

class InvGPT(nn.Module):
   def __init__(self, GPT_model):
        super().__init__()
        self.GPT_model = GPT_model

   def forward(self, input_vec, targets=None):
        device = input_vec.device
        t = input_vec.shape[1]
        assert t <= self.GPT_model.block_size, f"Cannot forward sequence of length {t}, block size is only {self.GPT_model.block_size}"
        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)

        # forward the GPT model itself
        # tok_emb = self.GPT_model.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        pos_emb = self.GPT_model.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)

        x = self.GPT_model.transformer.drop(input_vec + pos_emb)
        for block in self.GPT_model.transformer.h:
            x = block(x)
        x = self.GPT_model.transformer.ln_f(x)
        logits = self.GPT_model.lm_head(x)
        # if we are given some desired targets also calculate the loss
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        return logits, loss

"""# Q1 Inversion"""

def train_inversion():
    path_trained_model = r"/content/drive/MyDrive/DL course/EX1/trained models/model_gpt2_iters_10000.pth"
    sentece = "I am a little squirrel holding a walnut"
    steps = 400
    EMBED_DIM = 768
    model_trained = load_gpt_model()
    model_trained.load_state_dict(torch.load(path_trained_model))
    model_trained.eval()
    inv_gpt = InvGPT(model_trained)
    tokenizer = bpe.BPETokenizer()
    encoder = bpe.get_encoder()
    target = torch.tensor(encoder.encode(sentece)) 
    T = target.shape[-1]
    input_vec = torch.randn(1, T, EMBED_DIM, requires_grad=True)
    print(f"the shape of the input vector is: {input_vec.shape}")
    # Define the optimizer
    optimizer = torch.optim.Adam([input_vec], lr=0.1)
    loss_list = []
    # Optimize z to minimize the difference between G(z) and I
    for step in range(steps):
        optimizer.zero_grad()
        logits, loss = inv_gpt(input_vec=input_vec, targets=target)
        loss.backward()
        optimizer.step()
        loss_list.append(loss.item())
        output_idx = torch.argmax(logits, dim=-1) 
        output_idx = output_idx.flatten() # shape (t)
        output_text = tokenizer.decode(output_idx)
        print(f"step {step}: {output_text}")
        if output_text == sentece:
          break 
    plot_loss(loss_list)

train_inversion()

"""# Q2 Attention scores"""

import math

class CustomAttn(nn.Module):
    """
    A vanilla multi-head masked self-attention layer with a projection at the end.
    It is possible to use torch.nn.MultiheadAttention here but I am including an
    explicit implementation here to show that there is nothing too scary here.
    """

    def __init__(self, attn_module):
        super().__init__()
        self.attn = attn_module

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k ,v  = self.attn.c_attn(x).split(self.attn.n_embd, dim=2)
        k = k.view(B, T, self.attn.n_head, C // self.attn.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.attn.n_head, C // self.attn.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.attn.n_head, C // self.attn.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.attn.bias[:,:,:T,:T] == 0, float('-inf'))  # check this out
        att = F.softmax(att, dim=-1)
        attn_to_save = att
        att = self.attn.attn_dropout(att)
        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
        # output projection
        y = self.attn.resid_dropout(self.attn.c_proj(y))
        return y, attn_to_save

class CustomBlock(nn.Module):
    """ an unassuming Transformer block """
    def __init__(self, block_module):
        super().__init__()
        block_module.attn = CustomAttn(block_module.attn)
        self.block = block_module  # switch the previus attention with new one

    def forward(self, x):
        y, attn_to_save = self.block.attn(self.block.ln_1(x))
        x = x + y
        x = x + self.block.mlpf(self.block.ln_2(x))
        return x, attn_to_save

class CustomGPT(nn.Module):
    """ GPT Language Model """
    def __init__(self, gpt_module):
        super().__init__()
        self.gpt = gpt_module
        self.gpt.transformer.h = nn.ModuleList([CustomBlock(block) for block in self.gpt.transformer.h])

    def forward(self, idx, targets=None):
          device = idx.device
          b, t = idx.size()
          assert t <= self.gpt.block_size, f"Cannot forward sequence of length {t}, block size is only {self.gpt.block_size}"
          pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)

          # forward the GPT model itself
          tok_emb = self.gpt.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
          pos_emb = self.gpt.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)
          x = self.gpt.transformer.drop(tok_emb + pos_emb)
          attentions = []
          for i, block in enumerate(self.gpt.transformer.h):
              x, attn_to_save = block(x)
              if i == 0 or i == len(self.gpt.transformer.h) - 1:  # append only first and last attentions
                attentions.append(attn_to_save)
          x = self.gpt.transformer.ln_f(x)
          logits = self.gpt.lm_head(x)

          # if we are given some desired targets also calculate the loss
          loss = None
          if targets is not None:
              loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
          return logits, loss, attentions

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):
        """
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """
        all_attentions = []
        all_probs = []
        for i in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) <= self.gpt.block_size else idx[:, -self.gpt.block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, loss, attentions = self(idx_cond)
            all_attentions.append(attentions)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            all_probs.append(torch.max(probs))
            # either sample from the distribution or take the most likely element
            if do_sample:
                idx_next = torch.multinomial(probs, num_samples=1)
            else:
                _, idx_next = torch.topk(probs, k=1, dim=-1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx, all_attentions, all_probs


path_trained_model = r"/content/drive/MyDrive/DL course/EX1/trained models/model_gpt2_iters_5000.pth"
model_trained = load_gpt_model()
model_trained.load_state_dict(torch.load(path_trained_model))
model_trained.eval()
custom_gpt = CustomGPT(model_trained)

custom_gpt.eval()

def generate(model, prompt='', num_samples=1, steps=50, do_sample=True):
    tokenizer = mingpt.bpe.BPETokenizer()
    # tokenize the input prompt into integer input sequence
    if prompt == '':
        # manually create a tensor with only the special <|endoftext|> token
        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)
    else:
        x = tokenizer(prompt)
    # we'll process all desired num_samples in a batch, so expand out the batch dim
    x = x.expand(num_samples, -1)
    # forward the model `steps` times to get samples, in a batch
    y, all_attentions, all_probs = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)
    # attentions: [word0, word1, ...., word11[attnlyr0, ...,attnlyr11], ...]
    for i in range(num_samples):
        out = tokenizer.decode(y[i].cpu().squeeze())
        print('-'*80)
        print(out)
    return out, all_attentions, all_probs

"""# attentions"""

from contextlib import AbstractContextManager
import cv2
import numpy as np
def display_attention(out, all_attentions):
    word_num = 11
    first_attn_layer = all_attentions[word_num][0][0].mean(dim=0)
    last_attn_layer = all_attentions[word_num][1][0].mean(dim=0)
    S = out.split()[:word_num+1]
    A = last_attn_layer[word_num-1]
    words = S
    prob = A
    white = np.array([255, 255, 255])
    # create an image of white background
    font = cv2.FONT_HERSHEY_SIMPLEX
    fontScale = 1
    thickness = 2
    img = np.ones((100, 1500, 3), np.uint8)
    c1 = np.array([255., 255., 255.])
    c2 = np.array([0., 255., 0.])
    x = 0
    for i, word in enumerate(words):
        alpha = prob[i].item()
        print(alpha)
        c = 255 - (alpha * c1 + (1-alpha)**40 * c2)
        img = cv2.putText(img, word + " ", (x + 50, 50), font, fontScale, c, thickness, cv2.LINE_AA)
        x += cv2.getTextSize(word + " ", font, fontScale, thickness)[0][0]

    plt.imshow(img)
    plt.show()
prompt = "Either the well was very deep, or"
out, all_attentions, all_probs = generate(model=custom_gpt, steps=50, prompt=prompt)
display_attention(out, all_attentions)

"""# sentence probability"""

prompt = "Alice had learnt several things"
out, all_attentions, all_probs = generate(model=custom_gpt, steps=20, prompt=prompt)

# calculate log probabilities :
torch.log(torch.tensor(all_probs.numpy())).sum()

print(np.array(all_probs))
torch.log(torch.tensor(all_probs)).sum()